{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing CSCDataset from `anndata.experimental` is deprecated. Import anndata.abc.CSCDataset instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing CSRDataset from `anndata.experimental` is deprecated. Import anndata.abc.CSRDataset instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_elem from `anndata.experimental` is deprecated. Import anndata.io.read_elem instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "seed = 42\n",
    "scvi.settings.seed = seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"data/raw-count-full-genes-with-cell-type-annotation.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 107974 × 36601\n",
       "    obs: 'sample_batch', 'initial_size_unspliced', 'initial_size_spliced', 'initial_size', 'dataset', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'doublet_scores', 'predicted_doublets', 'n_counts', 'n_genes', 'sample description', 'experiment', 'patient', 'level1', 'level2', 'level3', 'sample'\n",
       "    var: 'gene_ids', 'feature_types', 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'mt'\n",
       "    layers: 'ambiguous', 'raw', 'spliced', 'unspliced'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neCRSwNP_2',\n",
       " 'eCRSwNP_2',\n",
       " 'Control_1',\n",
       " 'Control_3',\n",
       " 'neCRSwNP_4',\n",
       " 'neCRSwNP_5',\n",
       " 'eCRSwNP_3',\n",
       " 'eCRSwNP_4',\n",
       " 'eCRSwNP_5',\n",
       " 'Control_4',\n",
       " 'Control_5',\n",
       " 'Control_6',\n",
       " 'Control_7',\n",
       " 'Control_8']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs['dataset'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_anndata(adata):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    adata_cleaned = adata.copy()\n",
    "    \n",
    "    # Check and clean raw layer\n",
    "    if 'raw' in adata_cleaned.layers:\n",
    "        # Replace inf and -inf with NaN\n",
    "        raw_layer = adata_cleaned.layers['raw'].copy()\n",
    "        \n",
    "        if sp.issparse(raw_layer):\n",
    "            # For sparse matrices\n",
    "            raw_layer.data[np.isinf(raw_layer.data)] = 0\n",
    "            raw_layer.data[np.isnan(raw_layer.data)] = 0\n",
    "        else:\n",
    "            # For dense arrays\n",
    "            raw_layer[np.isinf(raw_layer)] = 0\n",
    "            raw_layer[np.isnan(raw_layer)] = 0\n",
    "        \n",
    "        adata_cleaned.layers['raw'] = raw_layer\n",
    "    \n",
    "    # Clean X layer if needed\n",
    "    if sp.issparse(adata_cleaned.X):\n",
    "        adata_cleaned.X.data[np.isinf(adata_cleaned.X.data)] = 0\n",
    "        adata_cleaned.X.data[np.isnan(adata_cleaned.X.data)] = 0\n",
    "    else:\n",
    "        adata_cleaned.X[np.isinf(adata_cleaned.X)] = 0\n",
    "        adata_cleaned.X[np.isnan(adata_cleaned.X)] = 0\n",
    "    \n",
    "    return adata_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_scanvi_data(adata, cell_type_levels=['level1', 'level2', 'level3'], n_top_genes=4000):\n",
    "    # Clean the AnnData object first\n",
    "    adata_cleaned = clean_anndata(adata)\n",
    "    \n",
    "    # Ensure raw layer exists\n",
    "    if 'raw' not in adata_cleaned.layers:\n",
    "        adata_cleaned.layers['raw'] = adata_cleaned.X.copy()\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    adata_processed = adata_cleaned.copy()\n",
    "    \n",
    "    # Ensure X is using raw counts\n",
    "    adata_processed.X = adata_processed.layers['raw'].copy()\n",
    "    \n",
    "    # Preprocessing steps\n",
    "    sc.pp.filter_cells(adata_processed, min_genes=200)\n",
    "    sc.pp.filter_genes(adata_processed, min_cells=3)\n",
    "    \n",
    "    # Log normalize and find variable genes\n",
    "    sc.pp.normalize_total(adata_processed, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_processed)\n",
    "    \n",
    "    # Find highly variable genes with robust method\n",
    "    sc.pp.highly_variable_genes(\n",
    "        adata_processed, \n",
    "        n_top_genes=n_top_genes, \n",
    "        min_mean=0.0125, \n",
    "        max_mean=3, \n",
    "        min_disp=0.5\n",
    "    )\n",
    "    \n",
    "    # Subset to highly variable genes\n",
    "    adata_processed = adata_processed[:, adata_processed.var['highly_variable']]\n",
    "    \n",
    "    # Prepare label encoders\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for level in cell_type_levels:\n",
    "        # Check if the level exists in obs\n",
    "        if level not in adata_processed.obs.columns:\n",
    "            print(f\"Warning: {level} not found in observation columns\")\n",
    "            continue\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        if pd.api.types.is_categorical_dtype(adata_processed.obs[level]):\n",
    "        # Add 'Unknown' to the categories if it's not already present\n",
    "            adata_processed.obs[level] = adata_processed.obs[level].cat.add_categories(['Unknown'])\n",
    "        # Fill NaN values with 'Unknown'\n",
    "        adata_processed.obs[level] = adata_processed.obs[level].fillna('Unknown')\n",
    "\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        adata_processed.obs[f'{level}_encoded'] = le.fit_transform(\n",
    "            adata_processed.obs[level]\n",
    "        )\n",
    "        label_encoders[level] = le\n",
    "    \n",
    "    return adata_processed, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scanvi_model(adata_processed, label_encoders, cell_type_levels=['level1', 'level2', 'level3']):\n",
    "    # Prepare results dictionary\n",
    "    results = {}\n",
    "   \n",
    "    # Train scANVI for each cell type level\n",
    "    for level in cell_type_levels:\n",
    "        print(f\"\\nTraining scANVI for {level}\")\n",
    "       \n",
    "        # Prepare the data\n",
    "        adata_level = adata_processed.copy()\n",
    "       \n",
    "        # Convert to string and then categorical if necessary\n",
    "        adata_level.obs[f'{level}_encoded'] = adata_level.obs[f'{level}_encoded'].astype(str)\n",
    "        if not pd.api.types.is_categorical_dtype(adata_level.obs[f'{level}_encoded']):\n",
    "            adata_level.obs[f'{level}_encoded'] = adata_level.obs[f'{level}_encoded'].astype('category')\n",
    "       \n",
    "        # Add \"Unknown\" to the categories for encoded labels if not already present\n",
    "        if 'Unknown' not in adata_level.obs[f'{level}_encoded'].cat.categories:\n",
    "            adata_level.obs[f'{level}_encoded'] = adata_level.obs[f'{level}_encoded'].cat.add_categories(['Unknown'])\n",
    "       \n",
    "        # Update label encoder to include \"Unknown\"\n",
    "        if 'Unknown' not in label_encoders[level].classes_:\n",
    "            new_classes = list(label_encoders[level].classes_) + ['Unknown']\n",
    "            label_encoders[level].classes_ = np.array(new_classes)\n",
    "       \n",
    "        # Setup scVI model\n",
    "        scvi.model.SCVI.setup_anndata(\n",
    "            adata_level,\n",
    "            layer='raw',\n",
    "            labels_key=f'{level}_encoded'\n",
    "        )\n",
    "       \n",
    "        # Initialize and train scVI model\n",
    "        vae = scvi.model.SCVI(\n",
    "            adata_level,\n",
    "            n_layers=2,\n",
    "            n_latent=30,\n",
    "            dropout_rate=0.2\n",
    "        )\n",
    "        vae.train(max_epochs=100, early_stopping=True)\n",
    "       \n",
    "        # Initialize and train scANVI model\n",
    "        scanvi = scvi.model.SCANVI.from_scvi_model(\n",
    "            vae,\n",
    "            labels_key=f'{level}_encoded',\n",
    "            unlabeled_category='Unknown'\n",
    "        )\n",
    "        scanvi.train(max_epochs=100, early_stopping=True)\n",
    "       \n",
    "        # Get predicted labels \n",
    "        # Use max probability to determine the predicted label\n",
    "        predictions_prob = scanvi.predict_label(adata_level)\n",
    "        predictions = predictions_prob.argmax(axis=1)\n",
    "        \n",
    "        # Convert predictions to original label names\n",
    "        decoded_predictions = label_encoders[level].inverse_transform(predictions)\n",
    "       \n",
    "        # Compute classification metrics\n",
    "        true_labels = adata_level.obs[level]\n",
    "       \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, decoded_predictions))\n",
    "       \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        cm = confusion_matrix(true_labels, decoded_predictions)\n",
    "        unique_labels = label_encoders[level].classes_\n",
    "        sns.heatmap(cm, annot=True, fmt='d',\n",
    "                    xticklabels=unique_labels,\n",
    "                    yticklabels=unique_labels)\n",
    "        plt.title(f'Confusion Matrix - {level}')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "       \n",
    "        # Store results\n",
    "        results[level] = {\n",
    "            'model': scanvi,\n",
    "            'predictions': decoded_predictions,\n",
    "            'true_labels': true_labels,\n",
    "            'label_encoder': label_encoders[level]\n",
    "        }\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(results, adata_processed, cell_type_levels=['level1', 'level2', 'level3']):\n",
    "    for level in cell_type_levels:\n",
    "        # Get the trained scANVI model\n",
    "        scanvi = results[level]['model']\n",
    "        \n",
    "        # Get latent representation\n",
    "        latent = scanvi.get_latent_representation(adata_processed)\n",
    "        \n",
    "        # Create a new AnnData object with latent representation\n",
    "        adata_latent = sc.AnnData(X=latent)\n",
    "        adata_latent.obs[level] = adata_processed.obs[level]\n",
    "        \n",
    "        # Perform UMAP on latent space\n",
    "        sc.pp.neighbors(adata_latent)\n",
    "        sc.tl.umap(adata_latent)\n",
    "        \n",
    "        # Plot UMAP\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sc.pl.umap(adata_latent, color=level, palette='tab20', \n",
    "                   title=f'UMAP of Latent Space - {level}', \n",
    "                   show=True, \n",
    "                   save=f'_umap_{level}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_prep = clean_anndata(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2639/2084524920.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(adata_processed.obs[level]):\n",
      "/tmp/ipykernel_2639/2084524920.py:48: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata_processed.obs[level] = adata_processed.obs[level].cat.add_categories(['Unknown'])\n",
      "/tmp/ipykernel_2639/2084524920.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(adata_processed.obs[level]):\n",
      "/tmp/ipykernel_2639/2084524920.py:46: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(adata_processed.obs[level]):\n"
     ]
    }
   ],
   "source": [
    "cell_type_levels=['level1', 'level2', 'level3']\n",
    "adata_processed, label_encoders = preprocess_scanvi_data(\n",
    "        adata_prep, \n",
    "        cell_type_levels=cell_type_levels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training scANVI for level1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2639/3229418274.py:14: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(adata_level.obs[f'{level}_encoded']):\n",
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████| 100/100 [20:14<00:00, 11.05s/it, v_num=1, train_loss_step=758, train_loss_epoch=701]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████| 100/100 [20:14<00:00, 12.15s/it, v_num=1, train_loss_step=758, train_loss_epoch=701]\n",
      "\u001b[34mINFO    \u001b[0m Training for \u001b[1;36m100\u001b[0m epochs.                                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n",
      "/home/dhakal/anaconda3/envs/URT_NSCL/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100:  95%|█████████▌| 95/100 [52:00<02:44, 32.85s/it, v_num=1, train_loss_step=722, train_loss_epoch=705] \n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 725.767. Signaling Trainer to stop.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SCANVI' object has no attribute 'predict_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_scanvi_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_type_levels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mtrain_scanvi_model\u001b[0;34m(adata_processed, label_encoders, cell_type_levels)\u001b[0m\n\u001b[1;32m     48\u001b[0m scanvi\u001b[38;5;241m.\u001b[39mtrain(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Get predicted labels \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Use max probability to determine the predicted label\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m predictions_prob \u001b[38;5;241m=\u001b[39m \u001b[43mscanvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_label\u001b[49m(adata_level)\n\u001b[1;32m     53\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions_prob\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Convert predictions to original label names\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SCANVI' object has no attribute 'predict_label'"
     ]
    }
   ],
   "source": [
    "results = train_scanvi_model(adata_processed, label_encoders, cell_type_levels=cell_type_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URT_NSCL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
